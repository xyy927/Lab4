# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r5S2m5LdrCEv7QZccrJXm0Z6wVD4Rmw_
"""

import pandas as pd

# Load the datasets
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Display the first few rows of the train and test dataframes
train_df_head = train_df.head()
test_df_head = test_df.head()

train_df_head, test_df_head

def standard_scaler(data):
    # Mean and standard deviation of the data
    mean = data.mean()
    std = data.std()

    # Standard scaling
    scaled_data = (data - mean) / std

    return scaled_data, mean, std

numerical_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']

# Apply the custom standard scaler to the training data
scaled_features = {}
for feature in numerical_features:
    train_df[feature], mean, std = standard_scaler(train_df[feature])
    scaled_features[feature] = {'mean': mean, 'std': std}

# Now we need to apply the same scaling to the test data
for feature in numerical_features:
    mean = scaled_features[feature]['mean']
    std = scaled_features[feature]['std']
    test_df[feature] = (test_df[feature] - mean) / std

train_df[numerical_features].head(), test_df[numerical_features].head()

# Check for missing values in the training and test sets
missing_values_train = train_df[numerical_features].isnull().sum()
missing_values_test = test_df[numerical_features].isnull().sum()

missing_values_train, missing_values_test

# Impute the missing values using median for 'Age' and 'Fare'
train_df['Age'].fillna(train_df['Age'].median(), inplace=True)
test_df['Age'].fillna(train_df['Age'].median(), inplace=True)  # Use the training median for the test set
test_df['Fare'].fillna(train_df['Fare'].median(), inplace=True)

# Check again for missing values to confirm they have been filled
missing_values_train_after = train_df[numerical_features].isnull().sum()
missing_values_test_after = test_df[numerical_features].isnull().sum()

missing_values_train_after, missing_values_test_after

from sklearn.model_selection import train_test_split

# Define the features and target variable
X = train_df[numerical_features]  # Features
y = train_df['Survived']          # Target variable

# Split the data into training and validation sets (80% train, 20% validation)
X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_validation.shape, y_train.shape, y_validation.shape

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Range of k values to try
k_values = range(1, 50)
accuracies = []

# Loop over the range of k values
for k in k_values:
    # Initialize the KNN classifier with the current k value
    knn = KNeighborsClassifier(n_neighbors=k)

    # Fit the classifier to the training data
    knn.fit(X_train, y_train)

    # Predict on the validation set
    predictions = knn.predict(X_validation)

    # Calculate accuracy and append to list
    accuracy = accuracy_score(y_validation, predictions)
    accuracies.append(accuracy)

# Plotting the results
plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracies, marker='o')
plt.title('K-Value vs. Accuracy')
plt.xlabel('k-value')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()

# Best k value and the corresponding accuracy
best_k = k_values[accuracies.index(max(accuracies))]
best_accuracy = max(accuracies)

best_k, best_accuracy

from sklearn.model_selection import cross_val_score
import numpy as np

# Initialize the KNN classifier with the best k value
knn_best = KNeighborsClassifier(n_neighbors=best_k)

# Perform 5-fold cross-validation
cv_scores = cross_val_score(knn_best, X_train, y_train, cv=5)

# Mean and standard deviation of cross-validation scores
cv_mean = np.mean(cv_scores)
cv_std = np.std(cv_scores)

cv_mean, cv_std